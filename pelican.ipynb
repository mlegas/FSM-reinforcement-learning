{
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('RL_Project': conda)"
  },
  "interpreter": {
   "hash": "f6feb42cf36f43b71669a8398d59077f880f7d6df8170b1d736604ffc0ad9a83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "class FSMFileTransitionsError(Exception):\r\n",
    "    \r\n",
    "    def __init__(self, list_of_states):\r\n",
    "        print(\"States of the following unique IDs have an amount of 'possible discrete actions (transitions)' that is not equal to the amount of transitions in their 'Transition names' field.\")\r\n",
    "        print(*list_of_states)\r\n",
    "        print(\"Please recheck the states and try again.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "import pandas\r\n",
    "\r\n",
    "def convert_state_names(file_name):\r\n",
    "    df = pandas.read_csv(file_name)\r\n",
    "\r\n",
    "    if 'Discretized_state_name' not in df.columns:\r\n",
    "        state_names_dict = {}\r\n",
    "        counter = 1\r\n",
    "        for row in df.itertuples(index=False):\r\n",
    "            print(row)\r\n",
    "            if row.State_name not in state_names_dict:\r\n",
    "                state_names_dict[row.State_name] = counter\r\n",
    "                print(state_names_dict[row.State_name])\r\n",
    "                counter += 1\r\n",
    "        \r\n",
    "        df['Discretized_state_name'] = df['State_name'].map(state_names_dict)\r\n",
    "        df.to_csv(file_name, index=False)\r\n",
    "\r\n",
    "    else:\r\n",
    "        print('INFO: The file provided already contains discretized state names.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "convert_state_names('pelican.csv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO: The file provided already contains discretized state names.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "import numpy as np\r\n",
    "import gym\r\n",
    "from stable_baselines3.common.env_checker import check_env\r\n",
    "from csv import DictReader\r\n",
    "\r\n",
    "class FSMEnv(gym.Env):\r\n",
    "  \"\"\"\r\n",
    "  Custom Environment that follows gym interface.\r\n",
    "  This is a simple env where the agent must learn to go always left. \r\n",
    "  \"\"\"\r\n",
    "  # Because of google colab, we cannot implement the GUI ('human' render mode)\r\n",
    "  metadata = {'render.modes': ['console']}\r\n",
    "\r\n",
    "  def __init__(self, file_name, start_state_id=None):\r\n",
    "    super(FSMEnv, self).__init__()\r\n",
    "\r\n",
    "    # The name of the .csv file containing the FSM\r\n",
    "    self.file_name = file_name\r\n",
    "    self.start_state_id = start_state_id\r\n",
    "\r\n",
    "    convert_state_names(file_name)\r\n",
    "\r\n",
    "    if start_state_id is None:\r\n",
    "      preset_start_state = False\r\n",
    "    else:\r\n",
    "      preset_start_state = True\r\n",
    "      self.start_state_id = start_state_id\r\n",
    "\r\n",
    "    if preset_start_state == False:  \r\n",
    "      start_state_already_set = False\r\n",
    "      multiple_start_states = False\r\n",
    "\r\n",
    "      with open(self.file_name, 'r') as csv_file:\r\n",
    "        # pass the file object to DictReader() to get the DictReader object\r\n",
    "        csv_dict_reader = DictReader(csv_file)\r\n",
    "        start_states = []\r\n",
    "\r\n",
    "        # iterate over each line as a ordered dictionary\r\n",
    "        for row in csv_dict_reader:\r\n",
    "          # row variable is a dictionary that represents a row in csv\r\n",
    "          if row['Start_state'] == '1':\r\n",
    "            start_states.append(row['Unique_ID'])\r\n",
    "            \r\n",
    "            if start_state_already_set == False:\r\n",
    "              start_state_already_set = True\r\n",
    "              self.start_state_id = int(row['Unique_ID'])\r\n",
    "            else:\r\n",
    "              self.start_state_id = None\r\n",
    "              multiple_start_states = True\r\n",
    "      \r\n",
    "        if multiple_start_states == True:\r\n",
    "          print(\"Multiple start states have been found!\")\r\n",
    "          correct_start_state_provided = False\r\n",
    "\r\n",
    "          while correct_start_state_provided == False:\r\n",
    "            print(\"Please choose one of the found start states:\")\r\n",
    "            for unique_id in start_states:\r\n",
    "              print(unique_id)\r\n",
    "\r\n",
    "            self.start_state_id = input(\"Provide an unique ID: \")\r\n",
    "            \r\n",
    "            if self.start_state_id not in start_states:\r\n",
    "              print(\"ID provided is not in the list!\")\r\n",
    "            \r\n",
    "            else:\r\n",
    "              correct_start_state_provided = True\r\n",
    "        \r\n",
    "    row_counter = 0\r\n",
    "\r\n",
    "    # Define action and observation space\r\n",
    "    # They must be gym.spaces objects\r\n",
    "    # Example when using discrete actions, we have two: left and right\r\n",
    "    with open(self.file_name, 'r') as csv_file:\r\n",
    "      csv_dict_reader = DictReader(csv_file)\r\n",
    "      broken_states = []\r\n",
    "      broken_states_exist = False\r\n",
    "\r\n",
    "      for row in csv_dict_reader:\r\n",
    "        row_counter += 1\r\n",
    "\r\n",
    "        if len(row['Transitions_to_states'].split()) != int(row['Possible_discrete_actions_(transitions)']):\r\n",
    "          broken_states_exist = True\r\n",
    "          broken_states.append(row['Unique_ID'])\r\n",
    "\r\n",
    "        if row['Unique_ID'] == self.start_state_id:\r\n",
    "          initial_actions = int(row['Possible_discrete_actions_(transitions)'])\r\n",
    "          self.action_space = gym.spaces.Discrete(initial_actions)\r\n",
    "    \r\n",
    "    if broken_states_exist == True:\r\n",
    "      raise FSMFileTransitionsError(broken_states)\r\n",
    "\r\n",
    "    self.past_states = np.zeros((100,), dtype=int)\r\n",
    "    self.amount_of_states_visited = 0\r\n",
    "    self.agent_pos = self.start_state_id\r\n",
    "    self.current_discretized_state = 0\r\n",
    "\r\n",
    "    # Dict observation space\r\n",
    "    self.observation_space = gym.spaces.Dict(\r\n",
    "    {\r\n",
    "        # Current state obs\r\n",
    "        'current_state': gym.spaces.Discrete(row_counter),\r\n",
    "        # Past states history obs\r\n",
    "        'past_states': gym.spaces.Box(low=0, high=row_counter, shape=(100,), dtype=np.uint8),\r\n",
    "        # Transitions to states ?\r\n",
    "        # 'transitions_to_states': spaces.Box(low=0, high=row_counter, shape=(100,), dtype=np.uint8),\r\n",
    "        # Amount of states already visited (could be garbage data?)\r\n",
    "        'amount_of_states': gym.spaces.Discrete(row_counter),\r\n",
    "    })\r\n",
    "\r\n",
    "  def reset(self):\r\n",
    "    \"\"\"\r\n",
    "    Important: the observation must be a numpy array\r\n",
    "    :return: (np.array) \r\n",
    "    \"\"\"\r\n",
    "    # Initialize the agent at the start state\r\n",
    "    self.past_states = np.zeros((100,), dtype=int)\r\n",
    "    self.amount_of_states_visited = 0\r\n",
    "    self.agent_pos = self.start_state_id\r\n",
    "\r\n",
    "    with open(self.file_name, 'r') as csv_file:\r\n",
    "      csv_dict_reader = DictReader(csv_file)\r\n",
    "      \r\n",
    "      for row in csv_dict_reader:\r\n",
    "        if self.start_state_id == int(row['Unique_ID']):\r\n",
    "          self.current_discretized_state = int(row['Discretized_state_name'])\r\n",
    "\r\n",
    "    observation = {\r\n",
    "      'current_state': self.current_discretized_state,\r\n",
    "      'past_states': self.past_states,\r\n",
    "      'amount_of_states': self.amount_of_states_visited, \r\n",
    "    }\r\n",
    "\r\n",
    "    return observation\r\n",
    "\r\n",
    "  def step(self, action):\r\n",
    "    if action == self.LEFT:\r\n",
    "      self.agent_pos -= 1\r\n",
    "    elif action == self.RIGHT:\r\n",
    "      self.agent_pos += 1\r\n",
    "    else:\r\n",
    "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\r\n",
    "\r\n",
    "    # Account for the boundaries of the grid\r\n",
    "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\r\n",
    "\r\n",
    "    # Are we at the left of the grid?\r\n",
    "    done = bool(self.agent_pos == 0)\r\n",
    "\r\n",
    "    # Null reward everywhere except when reaching the goal (left of the grid)\r\n",
    "    reward = 1 if self.agent_pos == 0 else 0\r\n",
    "\r\n",
    "    # Optionally we can pass additional info, we are not using that for now\r\n",
    "    info = {}\r\n",
    "\r\n",
    "    return np.array([self.agent_pos]).astype(np.float32), reward, done, info\r\n",
    "\r\n",
    "  def render(self, mode='console'):\r\n",
    "    if mode != 'console':\r\n",
    "      raise NotImplementedError()\r\n",
    "    # agent is represented as a cross, rest as a dot\r\n",
    "    print(\".\" * self.agent_pos, end=\"\")\r\n",
    "    print(\"x\", end=\"\")\r\n",
    "    print(\".\" * (self.grid_size - self.agent_pos))\r\n",
    "\r\n",
    "  def close(self):\r\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "env = FSMEnv('pelican.csv')\r\n",
    "# If the environment don't follow the interface, an error will be thrown\r\n",
    "check_env(env, warn=True)\r\n",
    "#print(env.start_state_id)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO: The file provided already contains discretized state names.\n",
      "Multiple start states have been found!\n",
      "Please choose one of the found start states:\n",
      "1\n",
      "2\n",
      "4\n",
      "6\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'current_state' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-4186bc5696cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFSMEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pelican.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# If the environment don't follow the interface, an error will be thrown\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcheck_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(env.start_state_id)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\anaconda3\\envs\\RL_Project\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py\u001b[0m in \u001b[0;36mcheck_env\u001b[1;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[1;31m# ============ Check the returned values ===============\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m     \u001b[0m_check_returned_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;31m# ==== Check the render method and the declared render modes ====\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\anaconda3\\envs\\RL_Project\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py\u001b[0m in \u001b[0;36m_check_returned_values\u001b[1;34m(env, observation_space, action_space)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \"\"\"\n\u001b[0;32m    141\u001b[0m     \u001b[1;31m# because env inherits from gym.Env, we assume that `reset()` and `step()` methods exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-dd7c36a1b859>\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     observation = {\n\u001b[1;32m--> 128\u001b[1;33m       \u001b[1;34m'current_state'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;34m'past_states'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpast_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m       \u001b[1;34m'amount_of_states'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamount_of_states_visited\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'current_state' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = gym.make('CartPole-v0')\r\n",
    "env.reset()\r\n",
    "for _ in range(1000):\r\n",
    "    env.render()\r\n",
    "    env.step(env.action_space.sample()) # take a random action\r\n",
    "env.close()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-41ec89b51482>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# take a random action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\anaconda3\\envs\\RL_Project\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\anaconda3\\envs\\RL_Project\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\anaconda3\\envs\\RL_Project\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\anaconda3\\envs\\RL_Project\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_always_dwm\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dwm_composition_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                     \u001b[0m_dwmapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDwmFlush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gym\r\n",
    "\r\n",
    "from stable_baselines3 import PPO\r\n",
    "\r\n",
    "env = gym.make('CartPole-v0')\r\n",
    "\r\n",
    "model = PPO('MlpPolicy', env, verbose=1)\r\n",
    "model.learn(total_timesteps=10000)\r\n",
    "\r\n",
    "obs = env.reset()\r\n",
    "for i in range(1000):\r\n",
    "    action, _state = model.predict(obs, deterministic=True)\r\n",
    "    obs, reward, done, info = env.step(action)\r\n",
    "    env.render()\r\n",
    "    if done:\r\n",
    "      obs = env.reset()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.8     |\n",
      "|    ep_rew_mean     | 23.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2697     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = GoLeftEnv(grid_size=10)\r\n",
    "\r\n",
    "obs = env.reset()\r\n",
    "env.render()\r\n",
    "\r\n",
    "print(env.observation_space)\r\n",
    "print(env.action_space)\r\n",
    "print(env.action_space.sample())\r\n",
    "\r\n",
    "GO_LEFT = 0\r\n",
    "# Hardcoded best agent: always go left!\r\n",
    "n_steps = 20\r\n",
    "for step in range(n_steps):\r\n",
    "  print(\"Step {}\".format(step + 1))\r\n",
    "  obs, reward, done, info = env.step(GO_LEFT)\r\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\r\n",
    "  env.render()\r\n",
    "  if done:\r\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\r\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'GoLeftEnv' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5c1462cad7a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGoLeftEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GoLeftEnv' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from stable_baselines3 import DQN, PPO, A2C\r\n",
    "from stable_baselines3.common.cmd_util import make_vec_env\r\n",
    "\r\n",
    "# Instantiate the env\r\n",
    "env = GoLeftEnv(grid_size=10)\r\n",
    "# wrap it\r\n",
    "env = make_vec_env(lambda: env, n_envs=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Programy\\anaconda3\\envs\\RL_Project\\lib\\site-packages\\stable_baselines3\\common\\cmd_util.py:5: FutureWarning: Module ``common.cmd_util`` has been renamed to ``common.env_util`` and will be removed in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train the agent\r\n",
    "model = PPO('MlpPolicy', env, verbose=1).learn(5000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 138      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 17244    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 552      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 161      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 17428    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 1290     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 159      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 17454    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 1903     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 143      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 17331    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 2286     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 163      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 17479    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 3267     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 149      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 17335    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 3587     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 146      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 17275    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 4093     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 151      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 17396    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 4835     |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test the trained agent\r\n",
    "obs = env.reset()\r\n",
    "n_steps = 20\r\n",
    "for step in range(n_steps):\r\n",
    "  action, _ = model.predict(obs, deterministic=True)\r\n",
    "  print(\"Step {}\".format(step + 1))\r\n",
    "  print(\"Action: \", action)\r\n",
    "  obs, reward, done, info = env.step(action)\r\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\r\n",
    "  env.render(mode='console')\r\n",
    "  if done:\r\n",
    "    # Note that the VecEnv resets automatically\r\n",
    "    # when a done signal is encountered\r\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\r\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 1\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 2\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 3\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 4\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 5\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 6\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 7\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 8\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 9\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 10\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 11\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 12\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 13\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 14\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 15\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 16\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 17\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 18\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 19\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 20\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}