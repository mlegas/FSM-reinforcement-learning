{
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('RL_Project': conda)"
  },
  "interpreter": {
   "hash": "f6feb42cf36f43b71669a8398d59077f880f7d6df8170b1d736604ffc0ad9a83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import numpy as np\r\n",
    "import gym\r\n",
    "from stable_baselines3.common.env_checker import check_env\r\n",
    "from csv import DictReader\r\n",
    "\r\n",
    "class FSMEnv(gym.Env):\r\n",
    "  \"\"\"\r\n",
    "  Custom Environment that follows gym interface.\r\n",
    "  This is a simple env where the agent must learn to go always left. \r\n",
    "  \"\"\"\r\n",
    "  # Because of google colab, we cannot implement the GUI ('human' render mode)\r\n",
    "  metadata = {'render.modes': ['console']}\r\n",
    "\r\n",
    "  def __init__(self, file_name, start_state_id=None):\r\n",
    "    super(FSMEnv, self).__init__()\r\n",
    "\r\n",
    "    # The name of the .csv file containing the FSM\r\n",
    "    self.file_name = file_name\r\n",
    "    self.start_state_id = start_state_id\r\n",
    "\r\n",
    "    if start_state_id is None:\r\n",
    "      preset_start_state = False\r\n",
    "    else:\r\n",
    "      preset_start_state = True\r\n",
    "      self.start_state_id = start_state_id\r\n",
    "\r\n",
    "    if preset_start_state == False:  \r\n",
    "      start_state_already_set = False\r\n",
    "      multiple_start_states = False\r\n",
    "\r\n",
    "      with open(self.file_name, 'r') as file:\r\n",
    "        # pass the file object to DictReader() to get the DictReader object\r\n",
    "        csv_dict_reader = DictReader(file)\r\n",
    "        # iterate over each line as a ordered dictionary\r\n",
    "        for row in csv_dict_reader:\r\n",
    "          # row variable is a dictionary that represents a row in csv\r\n",
    "          if row['Start state'] == 1:\r\n",
    "            if start_state_already_set == False:\r\n",
    "              start_state_already_set = True\r\n",
    "              self.start_state_id = row['Unique ID']\r\n",
    "            else:\r\n",
    "              multiple_start_states = True\r\n",
    "              start_states = []\r\n",
    "              start_states.append(row['Unique ID'])\r\n",
    "      \r\n",
    "        if multiple_start_states == True:\r\n",
    "          print(\"Multiple start states have been found!\")\r\n",
    "          correct_start_state_provided = False\r\n",
    "\r\n",
    "          while correct_start_state_provided == False:\r\n",
    "            print(\"Please choose one of the found start states:\")\r\n",
    "            for id in start_states:\r\n",
    "              print(id)\r\n",
    "\r\n",
    "            start_state_id = input(\"Provide an unique ID: \")\r\n",
    "            if start_state_id not in start_states:\r\n",
    "              print(\"ID provided not in the list!\")\r\n",
    "            \r\n",
    "            else:\r\n",
    "              self.start_state_id = start_state_id\r\n",
    "              correct_start_state_provided = True            \r\n",
    "\r\n",
    "    # Define action and observation space\r\n",
    "    # They must be gym.spaces objects\r\n",
    "    # Example when using discrete actions, we have two: left and right\r\n",
    "    n_actions = 2\r\n",
    "    self.action_space = spaces.Discrete(n_actions)\r\n",
    "    # The observation will be the coordinate of the agent\r\n",
    "    # this can be described both by Discrete and Box space\r\n",
    "    self.observation_space = 0\r\n",
    "\r\n",
    "  def reset(self):\r\n",
    "    \"\"\"\r\n",
    "    Important: the observation must be a numpy array\r\n",
    "    :return: (np.array) \r\n",
    "    \"\"\"\r\n",
    "    # Initialize the agent at the right of the grid\r\n",
    "    self.agent_pos = self.grid_size - 1\r\n",
    "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\r\n",
    "    return np.array([self.agent_pos]).astype(np.float32)\r\n",
    "\r\n",
    "  def step(self, action):\r\n",
    "    if action == self.LEFT:\r\n",
    "      self.agent_pos -= 1\r\n",
    "    elif action == self.RIGHT:\r\n",
    "      self.agent_pos += 1\r\n",
    "    else:\r\n",
    "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\r\n",
    "\r\n",
    "    # Account for the boundaries of the grid\r\n",
    "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\r\n",
    "\r\n",
    "    # Are we at the left of the grid?\r\n",
    "    done = bool(self.agent_pos == 0)\r\n",
    "\r\n",
    "    # Null reward everywhere except when reaching the goal (left of the grid)\r\n",
    "    reward = 1 if self.agent_pos == 0 else 0\r\n",
    "\r\n",
    "    # Optionally we can pass additional info, we are not using that for now\r\n",
    "    info = {}\r\n",
    "\r\n",
    "    return np.array([self.agent_pos]).astype(np.float32), reward, done, info\r\n",
    "\r\n",
    "  def render(self, mode='console'):\r\n",
    "    if mode != 'console':\r\n",
    "      raise NotImplementedError()\r\n",
    "    # agent is represented as a cross, rest as a dot\r\n",
    "    print(\".\" * self.agent_pos, end=\"\")\r\n",
    "    print(\"x\", end=\"\")\r\n",
    "    print(\".\" * (self.grid_size - self.agent_pos))\r\n",
    "\r\n",
    "  def close(self):\r\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "env = gym.make('CartPole-v0')\r\n",
    "env.reset()\r\n",
    "for _ in range(1000):\r\n",
    "    env.render()\r\n",
    "    env.step(env.action_space.sample()) # take a random action\r\n",
    "env.close()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-41ec89b51482>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# take a random action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\anaconda3\\envs\\RL_Project\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\anaconda3\\envs\\RL_Project\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\anaconda3\\envs\\RL_Project\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\anaconda3\\envs\\RL_Project\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_always_dwm\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dwm_composition_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                     \u001b[0m_dwmapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDwmFlush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import gym\r\n",
    "\r\n",
    "from stable_baselines3 import PPO\r\n",
    "\r\n",
    "env = gym.make('CartPole-v0')\r\n",
    "\r\n",
    "model = PPO('MlpPolicy', env, verbose=1)\r\n",
    "model.learn(total_timesteps=10000)\r\n",
    "\r\n",
    "obs = env.reset()\r\n",
    "for i in range(1000):\r\n",
    "    action, _state = model.predict(obs, deterministic=True)\r\n",
    "    obs, reward, done, info = env.step(action)\r\n",
    "    env.render()\r\n",
    "    if done:\r\n",
    "      obs = env.reset()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.8     |\n",
      "|    ep_rew_mean     | 23.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2697     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "env = FSMEnv('pelican.csv')\r\n",
    "# If the environment don't follow the interface, an error will be thrown\r\n",
    "check_env(env, warn=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Unique ID': '0', 'State name': 'S0', 'Start state': '1', 'Possible discrete actions (transitions)': '2', 'Transition names': '0 1', 'Transitions to states': '0 1'}\n",
      "{'Unique ID': '1', 'State name': 'S1', 'Start state': '1', 'Possible discrete actions (transitions)': '2', 'Transition names': '0 1', 'Transitions to states': '4 2'}\n",
      "{'Unique ID': '2', 'State name': 'S2', 'Start state': '0', 'Possible discrete actions (transitions)': '2', 'Transition names': '0 1', 'Transitions to states': '5 3'}\n",
      "{'Unique ID': '3', 'State name': 'S3', 'Start state': '1', 'Possible discrete actions (transitions)': '2', 'Transition names': '0 1', 'Transitions to states': '4 2'}\n",
      "{'Unique ID': '4', 'State name': 'S4', 'Start state': '0', 'Possible discrete actions (transitions)': '2', 'Transition names': '0 1', 'Transitions to states': '5 3'}\n",
      "{'Unique ID': '5', 'State name': 'S5', 'Start state': '1', 'Possible discrete actions (transitions)': '2', 'Transition names': '0 1', 'Transitions to states': '0 1'}\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'spaces' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5eccffb42bd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFSMEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pelican.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# If the environment don't follow the interface, an error will be thrown\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcheck_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-3c589741fdb7>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file_name)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Example when using discrete actions, we have two: left and right\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mn_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;31m# The observation will be the coordinate of the agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# this can be described both by Discrete and Box space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spaces' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "env = GoLeftEnv(grid_size=10)\r\n",
    "\r\n",
    "obs = env.reset()\r\n",
    "env.render()\r\n",
    "\r\n",
    "print(env.observation_space)\r\n",
    "print(env.action_space)\r\n",
    "print(env.action_space.sample())\r\n",
    "\r\n",
    "GO_LEFT = 0\r\n",
    "# Hardcoded best agent: always go left!\r\n",
    "n_steps = 20\r\n",
    "for step in range(n_steps):\r\n",
    "  print(\"Step {}\".format(step + 1))\r\n",
    "  obs, reward, done, info = env.step(GO_LEFT)\r\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\r\n",
    "  env.render()\r\n",
    "  if done:\r\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\r\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".........x.\n",
      "Box(0.0, 10.0, (1,), float32)\n",
      "Discrete(2)\n",
      "1\n",
      "Step 1\n",
      "obs= [8.] reward= 0 done= False\n",
      "........x..\n",
      "Step 2\n",
      "obs= [7.] reward= 0 done= False\n",
      ".......x...\n",
      "Step 3\n",
      "obs= [6.] reward= 0 done= False\n",
      "......x....\n",
      "Step 4\n",
      "obs= [5.] reward= 0 done= False\n",
      ".....x.....\n",
      "Step 5\n",
      "obs= [4.] reward= 0 done= False\n",
      "....x......\n",
      "Step 6\n",
      "obs= [3.] reward= 0 done= False\n",
      "...x.......\n",
      "Step 7\n",
      "obs= [2.] reward= 0 done= False\n",
      "..x........\n",
      "Step 8\n",
      "obs= [1.] reward= 0 done= False\n",
      ".x.........\n",
      "Step 9\n",
      "obs= [0.] reward= 1 done= True\n",
      "x..........\n",
      "Goal reached! reward= 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from stable_baselines3 import DQN, PPO, A2C\r\n",
    "from stable_baselines3.common.cmd_util import make_vec_env\r\n",
    "\r\n",
    "# Instantiate the env\r\n",
    "env = GoLeftEnv(grid_size=10)\r\n",
    "# wrap it\r\n",
    "env = make_vec_env(lambda: env, n_envs=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Programy\\anaconda3\\envs\\RL_Project\\lib\\site-packages\\stable_baselines3\\common\\cmd_util.py:5: FutureWarning: Module ``common.cmd_util`` has been renamed to ``common.env_util`` and will be removed in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Train the agent\r\n",
    "model = PPO('MlpPolicy', env, verbose=1).learn(5000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 138      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 17244    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 552      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 161      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 17428    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 1290     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 159      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 17454    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 1903     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 143      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 17331    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 2286     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 163      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 17479    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 3267     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 149      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 17335    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 3587     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 146      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 17275    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 4093     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 151      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 17396    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 4835     |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Test the trained agent\r\n",
    "obs = env.reset()\r\n",
    "n_steps = 20\r\n",
    "for step in range(n_steps):\r\n",
    "  action, _ = model.predict(obs, deterministic=True)\r\n",
    "  print(\"Step {}\".format(step + 1))\r\n",
    "  print(\"Action: \", action)\r\n",
    "  obs, reward, done, info = env.step(action)\r\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', done)\r\n",
    "  env.render(mode='console')\r\n",
    "  if done:\r\n",
    "    # Note that the VecEnv resets automatically\r\n",
    "    # when a done signal is encountered\r\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\r\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 1\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 2\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 3\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 4\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 5\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 6\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 7\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 8\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 9\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 10\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 11\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 12\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 13\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 14\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 15\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 16\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 17\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 18\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 19\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n",
      "Step 20\n",
      "Action:  [1]\n",
      "obs= [[10.]] reward= [0.] done= [False]\n",
      "..........x\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}